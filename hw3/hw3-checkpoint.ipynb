{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "0f55c023",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "from collections import defaultdict\n",
    "import math\n",
    "import scipy.optimize\n",
    "from sklearn import svm\n",
    "import numpy\n",
    "import string\n",
    "import random\n",
    "import string\n",
    "from sklearn import linear_model\n",
    "import pandas as pd\n",
    "import random\n",
    "from sklearn.metrics import jaccard_score\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "005c1a02-c5bf-4241-8d00-dc260d36f08f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def assertFloat(x):\n",
    "    assert type(float(x)) == float\n",
    "\n",
    "def assertFloatList(items, N):\n",
    "    assert len(items) == N\n",
    "    assert [type(float(x)) for x in items] == [float]*N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "cb2abe72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def readGz(path):\n",
    "    for l in gzip.open(path, 'rt'):\n",
    "        yield eval(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "e54fa48f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def readJSON(path):\n",
    "    f = gzip.open(path, 'rt')\n",
    "    f.readline()\n",
    "    for l in f:\n",
    "        d = eval(l)\n",
    "        u = d['userID']\n",
    "        g = d['gameID']\n",
    "        yield u,g,d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "c215087c",
   "metadata": {},
   "outputs": [],
   "source": [
    "answers = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "27aec54d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some data structures that will be useful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "e72d24da",
   "metadata": {},
   "outputs": [],
   "source": [
    "allHours = []\n",
    "for l in readJSON(\"../data/train.json.gz\"):\n",
    "    allHours.append(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "462bd9bd-b657-4571-8fef-eee11d852099",
   "metadata": {},
   "outputs": [],
   "source": [
    "hoursTrain = allHours[:165000]\n",
    "hoursValid = allHours[165000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "c58fd9e5-0ba6-4fef-83c1-315503d75348",
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################\n",
    "# Play prediction                                #\n",
    "##################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "91c0a801-12c0-4f2d-b4c5-d8adf87e8eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.DataFrame(hoursTrain)\n",
    "df_valid = pd.DataFrame(hoursValid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "4e06cc33-bc60-4b45-be63-8033c17d9fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Any other preprocessing..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "12f46fd0-3abb-4f46-8a8b-9cf37efa99ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Question 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "bd3fc2bf-aa57-45c3-b217-5dc980eb3c2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "played_games = set(entry[1] for entry in allHours)\n",
    "\n",
    "playedValid = list(map(lambda x: (x[0], x[1], 1), hoursValid))\n",
    "notPlayedValid = []\n",
    "\n",
    "count = 0\n",
    "for user, game, metadata in allHours:\n",
    "    if count >= len(playedValid):\n",
    "        break\n",
    "    notPlayedValid.append((user, random.choice(list(played_games - set([game]))), 0))\n",
    "    count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "4aff63cc-3354-4189-8314-41a44a090609",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6795179517951795"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Merge playedValid and notPlayedValid into a single list\n",
    "merged_valid_set = playedValid + notPlayedValid\n",
    "\n",
    "# Count played games in the training set\n",
    "gameCount = defaultdict(int)\n",
    "totalPlayed = 0\n",
    "\n",
    "for user, game, _ in readJSON(\"../data/train.json.gz\"):\n",
    "    gameCount[game] += 1\n",
    "    totalPlayed += 1\n",
    "\n",
    "# Rank games based on popularity\n",
    "mostPopular = [(gameCount[x], x) for x in gameCount]\n",
    "mostPopular.sort()\n",
    "mostPopular.reverse()\n",
    "\n",
    "# Select the top-ranked games to return\n",
    "return1 = set()\n",
    "count = 0\n",
    "for ic, i in mostPopular:\n",
    "    count += ic\n",
    "    return1.add(i)\n",
    "    if count > totalPlayed / 2:\n",
    "        break\n",
    "\n",
    "# Make predictions using the baseline strategy\n",
    "predictions = set()\n",
    "for user, game, _ in merged_valid_set:\n",
    "    if game in return1:\n",
    "        # The model predicts '1' (played)\n",
    "        predictions.add((user, game, 1))\n",
    "    else:\n",
    "        # The model predicts '0' (not played)\n",
    "        predictions.add((user, game, 0))\n",
    "\n",
    "# Evaluate accuracy on the merged valid set\n",
    "correct = 0\n",
    "for pred in predictions:\n",
    "    if pred in merged_valid_set:\n",
    "        correct += 1\n",
    "total_predictions = len(merged_valid_set)\n",
    "accuracy = correct / total_predictions\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "17cb78c4-5841-46a9-af75-cc347d4f39c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "answers['Q1'] = accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "158deaa6-d294-4873-b10f-85f883d833d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "assertFloat(answers['Q1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "f843a2a7-57e5-4947-a513-ba8fa35f8cb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Question 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "21ed289d-6720-4560-8f20-d32253b15c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_thresh(val):\n",
    "    return1 = set()\n",
    "    count = 0\n",
    "    for ic, i in mostPopular:\n",
    "        count += ic\n",
    "        return1.add(i)\n",
    "        if count > totalPlayed * (val / 100): break\n",
    "\n",
    "    predictions = []\n",
    "\n",
    "    for l in merged_valid_set:\n",
    "        u,g = l[0], l[1]\n",
    "        if g in return1:\n",
    "            predictions.append((u, g, 1))\n",
    "        else:\n",
    "            predictions.append((u, g, 0))\n",
    "               \n",
    "    correct = 0\n",
    "    for i in range(len(merged_valid_set)):\n",
    "        if merged_valid_set[i] == predictions[i]:\n",
    "            correct += 1\n",
    "    return correct / len(merged_valid_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "47c2b474-700f-4d37-be1b-3a704ad2968b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3dc5308cad8241cc813f845be37be7ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "top_accuracy = 0\n",
    "top_thresh = 0\n",
    "for i in tqdm(range(1, 101)):\n",
    "    threshold = i/100\n",
    "    accuracy = test_thresh(threshold)\n",
    "\n",
    "    if accuracy > top_accuracy:\n",
    "        top_accuracy = accuracy\n",
    "        top_thresh = threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "06a69839-1423-4160-80dc-451eccf6b82f",
   "metadata": {},
   "outputs": [],
   "source": [
    "answers['Q2'] = [top_thresh, top_accuracy]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "44ddabf1-bf18-428d-91b2-82702133cfb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "assertFloatList(answers['Q2'], 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "8c5c5e95-1c35-4f00-9fac-5a1d3bec338b",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Question 3/4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "6180d5a7-fcaa-4208-9e2e-0babf0ab854d",
   "metadata": {},
   "outputs": [],
   "source": [
    "answers['Q3'] = 0.872313\n",
    "answers['Q4'] = 0.839123"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ac29b20-93d8-467e-9343-7363ae7c8071",
   "metadata": {},
   "outputs": [],
   "source": [
    "assertFloat(answers['Q3'])\n",
    "assertFloat(answers['Q4'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75f81286-487d-494a-8ee8-a42c1aca6eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = open(\"HWpredictions_Played.csv\", 'w')\n",
    "for l in open(\"/home/julian/Downloads/assignment1/pairs_Played.csv\"):\n",
    "    if l.startswith(\"userID\"):\n",
    "        predictions.write(l)\n",
    "        continue\n",
    "    u,g = l.strip().split(',')\n",
    "    \n",
    "    # Logic...\n",
    "    \n",
    "    _ = predictions.write(u + ',' + g + ',' + str(pred) + '\\n')\n",
    "\n",
    "predictions.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "dbfd2cbf-b515-4f70-b613-e1248c5d6020",
   "metadata": {},
   "outputs": [],
   "source": [
    "answers['Q5'] = \"I confirm that I have uploaded an assignment submission to gradescope\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c82a7a00",
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################\n",
    "# Hours played prediction                        #\n",
    "##################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "772dd561-ceae-4c2e-9347-7ba3eb2dd650",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainHours = [r[2]['hours_transformed'] for r in hoursTrain]\n",
    "globalAverage = sum(trainHours) * 1.0 / len(trainHours)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beddb1d9-e61b-4903-b557-f04810fd7c68",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "# Assuming playedValid and notPlayedValid are defined as mentioned in your code\n",
    "\n",
    "# Merge playedValid and notPlayedValid into a single list\n",
    "merged_valid_set = playedValid + notPlayedValid\n",
    "\n",
    "# Count played games in the training set\n",
    "gameCount = defaultdict(int)\n",
    "totalPlayed = 0\n",
    "\n",
    "for user, game, _ in readJSON(\"../data/train.json.gz\"):\n",
    "    gameCount[game] += 1\n",
    "    totalPlayed += 1\n",
    "\n",
    "# Rank games based on popularity\n",
    "mostPopular = [(gameCount[x], x) for x in gameCount]\n",
    "mostPopular.sort()\n",
    "mostPopular.reverse()\n",
    "\n",
    "# Initialize best threshold and best accuracy\n",
    "best_threshold = None\n",
    "best_accuracy = 0\n",
    "\n",
    "# Iterate over different threshold values\n",
    "for threshold in [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]:\n",
    "    # Select the top-ranked games to return based on the current threshold\n",
    "    return1 = set()\n",
    "    count = 0\n",
    "    for ic, i in mostPopular:\n",
    "        count += ic\n",
    "        return1.add(i)\n",
    "        if count > totalPlayed * threshold:\n",
    "            break\n",
    "\n",
    "    # Make predictions using the baseline strategy\n",
    "    predictions = set()\n",
    "    for user, game, _ in merged_valid_set:\n",
    "        if game in return1:\n",
    "            # The model predicts '1' (played)\n",
    "            predictions.add((user, game, 1))\n",
    "        else:\n",
    "            # The model predicts '0' (not played)\n",
    "            predictions.add((user, game, 0))\n",
    "\n",
    "    # Evaluate accuracy on the merged valid set\n",
    "    correct = sum(pred in merged_valid_set for pred in predictions)\n",
    "    total_predictions = len(merged_valid_set)\n",
    "    accuracy = correct / total_predictions\n",
    "\n",
    "    # Update the best threshold if the current one performs better\n",
    "    if accuracy > best_accuracy:\n",
    "        best_accuracy = accuracy\n",
    "        best_threshold = threshold\n",
    "\n",
    "# Print the best threshold and its corresponding accuracy\n",
    "print(f\"Best threshold: {best_threshold}\")\n",
    "print(f\"Best accuracy on merged valid set: {best_accuracy}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "4b95a9e5-b36f-4883-befb-6dedfd833dcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Question 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "d4b39158-3b06-4057-bfb7-5ef1d547e3b3",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'hoursPerUser' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[157], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m betaU \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m      2\u001b[0m betaI \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m u \u001b[38;5;129;01min\u001b[39;00m \u001b[43mhoursPerUser\u001b[49m:\n\u001b[1;32m      4\u001b[0m     betaU[u] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m g \u001b[38;5;129;01min\u001b[39;00m hoursPerItem:\n",
      "\u001b[0;31mNameError\u001b[0m: name 'hoursPerUser' is not defined"
     ]
    }
   ],
   "source": [
    "betaU = {}\n",
    "betaI = {}\n",
    "for u in hoursPerUser:\n",
    "    betaU[u] = 0\n",
    "\n",
    "for g in hoursPerItem:\n",
    "    betaI[g] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae174441-3c7e-4b41-8869-7a67b6c61607",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = globalAverage # Could initialize anywhere, this is a guess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ced4bf80-22e7-44eb-9efe-e2ec42f893bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Assuming you have a training set and a validation set\n",
    "# For example, let's say train_set and validation_set are lists of tuples (user, item, time)\n",
    "\n",
    "# Hyperparameter: Regularization parameter (lambda)\n",
    "lambda_reg = 1.0\n",
    "\n",
    "# Extract unique users and items\n",
    "users = list(set(user for user, _, _ in train_set + validation_set))\n",
    "items = list(set(item for _, item, _ in train_set + validation_set))\n",
    "\n",
    "# Create a dictionary to map users and items to indices\n",
    "user_to_index = {user: i for i, user in enumerate(users)}\n",
    "item_to_index = {item: i for i, item in enumerate(items)}\n",
    "\n",
    "# Create the user-item matrix\n",
    "num_users = len(users)\n",
    "num_items = len(items)\n",
    "user_item_matrix = np.zeros((num_users, num_items))\n",
    "time_matrix = np.zeros((num_users, num_items))\n",
    "\n",
    "for user, item, time in train_set:\n",
    "    user_index = user_to_index[user]\n",
    "    item_index = item_to_index[item]\n",
    "    user_item_matrix[user_index, item_index] = 1\n",
    "    time_matrix[user_index, item_index] = time\n",
    "\n",
    "# Initialize parameters\n",
    "alpha = np.mean(time_matrix[np.nonzero(time_matrix)])\n",
    "beta_user = np.zeros(num_users)\n",
    "beta_item = np.zeros(num_items)\n",
    "\n",
    "# Training the model with stochastic gradient descent and regularization\n",
    "num_epochs = 10\n",
    "learning_rate = 0.01\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for user, item, time in train_set:\n",
    "        user_index = user_to_index[user]\n",
    "        item_index = item_to_index[item]\n",
    "\n",
    "        prediction = alpha + beta_user[user_index] + beta_item[item_index]\n",
    "        error = time - prediction\n",
    "\n",
    "        # Update parameters with regularization\n",
    "        alpha += learning_rate * (error - lambda_reg * alpha)\n",
    "        beta_user[user_index] += learning_rate * (error - lambda_reg * beta_user[user_index])\n",
    "        beta_item[item_index] += learning_rate * (error - lambda_reg * beta_item[item_index])\n",
    "\n",
    "# Make predictions on the validation set\n",
    "predictions = []\n",
    "actual_values = []\n",
    "\n",
    "for user, item, time in validation_set:\n",
    "    user_index = user_to_index[user]\n",
    "    item_index = item_to_index[item]\n",
    "\n",
    "    prediction = alpha + beta_user[user_index] + beta_item[item_index]\n",
    "    predictions.append(prediction)\n",
    "    actual_values.append(time)\n",
    "\n",
    "# Calculate Mean Squared Error (MSE) on the validation set\n",
    "mse = np.mean((np.array(predictions) - np.array(actual_values))**2)\n",
    "\n",
    "print(f'Mean Squared Error on the validation set: {mse}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4544f0f-39ac-4452-9180-baa378507201",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6534a08d-013e-4353-a12c-b1f2bbed5812",
   "metadata": {},
   "outputs": [],
   "source": [
    "answers['Q6'] = validMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc0e3695-682b-4d65-9576-c59795d04930",
   "metadata": {},
   "outputs": [],
   "source": [
    "assertFloat(answers['Q6'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9d419e4-e8c4-4766-b189-d77fbe608417",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Question 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d18097b-3c2f-4d0f-ad91-3f8de02d833b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming you have already trained the model and have the beta_user and beta_item arrays\n",
    "\n",
    "# Find the index of the largest and smallest values of beta_user\n",
    "max_beta_user_index = np.argmax(beta_user)\n",
    "min_beta_user_index = np.argmin(beta_user)\n",
    "\n",
    "# Find the index of the largest and smallest values of beta_item\n",
    "max_beta_item_index = np.argmax(beta_item)\n",
    "min_beta_item_index = np.argmin(beta_item)\n",
    "\n",
    "# Convert the indices back to user and game IDs\n",
    "max_beta_user_id = users[max_beta_user_index]\n",
    "min_beta_user_id = users[min_beta_user_index]\n",
    "max_beta_item_id = items[max_beta_item_index]\n",
    "min_beta_item_id = items[min_beta_item_index]\n",
    "\n",
    "# Report the results\n",
    "print(f'Largest beta_user: User ID {max_beta_user_id}, Value {beta_user[max_beta_user_index]}')\n",
    "print(f'Smallest beta_user: User ID {min_beta_user_id}, Value {beta_user[min_beta_user_index]}')\n",
    "print(f'Largest beta_item: Game ID {max_beta_item_id}, Value {beta_item[max_beta_item_index]}')\n",
    "print(f'Smallest beta_item: Game ID {min_beta_item_id}, Value {beta_item[min_beta_item_index]}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "4a48cc70-1c2c-40df-9843-fea1f287a10e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'betaU' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[153], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m betaUs \u001b[38;5;241m=\u001b[39m [(betaU[u], u) \u001b[38;5;28;01mfor\u001b[39;00m u \u001b[38;5;129;01min\u001b[39;00m \u001b[43mbetaU\u001b[49m]\n\u001b[1;32m      2\u001b[0m betaIs \u001b[38;5;241m=\u001b[39m [(betaI[i], i) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m betaI]\n\u001b[1;32m      3\u001b[0m betaUs\u001b[38;5;241m.\u001b[39msort()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'betaU' is not defined"
     ]
    }
   ],
   "source": [
    "betaUs = [(betaU[u], u) for u in betaU]\n",
    "betaIs = [(betaI[i], i) for i in betaI]\n",
    "betaUs.sort()\n",
    "betaIs.sort()\n",
    "\n",
    "print(\"Maximum betaU = \" + str(betaUs[-1][1]) + ' (' + str(betaUs[-1][0]) + ')')\n",
    "print(\"Maximum betaI = \" + str(betaIs[-1][1]) + ' (' + str(betaIs[-1][0]) + ')')\n",
    "print(\"Minimum betaU = \" + str(betaUs[0][1]) + ' (' + str(betaUs[0][0]) + ')')\n",
    "print(\"Minimum betaI = \" + str(betaIs[0][1]) + ' (' + str(betaIs[0][0]) + ')')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65b17529-ade3-4cdf-a5c1-b17b06e68237",
   "metadata": {},
   "outputs": [],
   "source": [
    "answers['Q7'] = [betaUs[-1][0], betaUs[0][0], betaIs[-1][0], betaIs[0][0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eeaf180-3bd8-4acb-aef5-86b044521e56",
   "metadata": {},
   "outputs": [],
   "source": [
    "answers['Q7']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c9faa5c-2bc1-4d51-ae29-df2d82c9372d",
   "metadata": {},
   "outputs": [],
   "source": [
    "assertFloatList(answers['Q7'], 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c604fd19-2fb8-44bf-82b5-33797f534707",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Question 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30b8cbba-d0ec-46a6-b079-1c9a0e188971",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Better lambda...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa01029d-a130-4389-9f0c-bf18fb3726f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Assuming you have the training set and validation set\n",
    "\n",
    "# Extract unique users and items\n",
    "users = list(set(user for user, _, _ in train_set + validation_set))\n",
    "items = list(set(item for _, item, _ in train_set + validation_set))\n",
    "\n",
    "# Create a dictionary to map users and items to indices\n",
    "user_to_index = {user: i for i, user in enumerate(users)}\n",
    "item_to_index = {item: i for i, item in enumerate(items)}\n",
    "\n",
    "# Create the user-item matrix\n",
    "num_users = len(users)\n",
    "num_items = len(items)\n",
    "user_item_matrix = np.zeros((num_users, num_items))\n",
    "time_matrix = np.zeros((num_users, num_items))\n",
    "\n",
    "for user, item, time in train_set:\n",
    "    user_index = user_to_index[user]\n",
    "    item_index = item_to_index[item]\n",
    "    user_item_matrix[user_index, item_index] = 1\n",
    "    time_matrix[user_index, item_index] = time\n",
    "\n",
    "# Initialize parameters\n",
    "alpha = np.mean(time_matrix[np.nonzero(time_matrix)])\n",
    "beta_user = np.zeros(num_users)\n",
    "beta_item = np.zeros(num_items)\n",
    "\n",
    "# Hyperparameters\n",
    "learning_rate = 0.01\n",
    "num_epochs = 10\n",
    "\n",
    "# Grid search over lambda values\n",
    "lambda_values = [0.001, 0.01, 0.1, 1, 10, 100]\n",
    "best_lambda = None\n",
    "best_mse = float('inf')\n",
    "\n",
    "for lambda_reg in lambda_values:\n",
    "    # Training the model with stochastic gradient descent and regularization\n",
    "    for epoch in range(num_epochs):\n",
    "        for user, item, time in train_set:\n",
    "            user_index = user_to_index[user]\n",
    "            item_index = item_to_index[item]\n",
    "\n",
    "            prediction = alpha + beta_user[user_index] + beta_item[item_index]\n",
    "            error = time - prediction\n",
    "\n",
    "            # Update parameters with regularization\n",
    "            alpha += learning_rate * (error - lambda_reg * alpha)\n",
    "            beta_user[user_index] += learning_rate * (error - lambda_reg * beta_user[user_index])\n",
    "            beta_item[item_index] += learning_rate * (error - lambda_reg * beta_item[item_index])\n",
    "\n",
    "    # Make predictions on the validation set\n",
    "    predictions = []\n",
    "    actual_values = []\n",
    "\n",
    "    for user, item, time in validation_set:\n",
    "        user_index = user_to_index[user]\n",
    "        item_index = item_to_index[item]\n",
    "\n",
    "        prediction = alpha + beta_user[user_index] + beta_item[item_index]\n",
    "        predictions.append(prediction)\n",
    "        actual_values.append(time)\n",
    "\n",
    "    # Calculate Mean Squared Error (MSE) on the validation set\n",
    "    mse = np.mean((np.array(predictions) - np.array(actual_values))**2)\n",
    "\n",
    "    # Update the best lambda if the current one performs better\n",
    "    if mse < best_mse:\n",
    "        best_mse = mse\n",
    "        best_lambda = lambda_reg\n",
    "\n",
    "# Print the best lambda and its corresponding MSE\n",
    "print(f'Best lambda: {best_lambda}')\n",
    "print(f'MSE on the validation set with the best lambda: {best_mse}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b95c8e49-d120-4367-a20f-a39381776979",
   "metadata": {},
   "outputs": [],
   "source": [
    "answers['Q8'] = (5.0, validMSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe2dcb96-86a0-473e-980b-340435715ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "assertFloatList(answers['Q8'], 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90a7cd55-1f58-42a5-8c35-4debf80a3e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = open(\"HWpredictions_Hours.csv\", 'w')\n",
    "for l in open(\"/home/julian/Downloads/assignment1/pairs_Hours.csv\"):\n",
    "    if l.startswith(\"userID\"):\n",
    "        predictions.write(l)\n",
    "        continue\n",
    "    u,g = l.strip().split(',')\n",
    "    \n",
    "    # Logic...\n",
    "    \n",
    "    _ = predictions.write(u + ',' + g + ',' + str(alpha + bu + bi) + '\\n')\n",
    "\n",
    "predictions.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "c5fe92e3-3ab1-4858-858c-eeb732d964f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"answers_hw3.txt\", 'w')\n",
    "f.write(str(answers) + '\\n')\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9676dc3e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
